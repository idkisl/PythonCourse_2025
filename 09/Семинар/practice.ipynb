{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfa62e2",
   "metadata": {},
   "source": [
    "# Введение в pandas\n",
    "\n",
    "`pandas` — мощная библиотека на Python для удобной работы с данными. Строится поверх NumPy и предлагает структуры данных: `Series` и `DataFrame`.\n",
    "\n",
    "* `Series`:\n",
    "  * Это одномерный массив с метками (индексом), способный хранить данные любых типов: числа, строки или даже объекты Python.\n",
    "  * Каждый элемент `Series` ассоциирован с индексом — можно думать о `Series` как о столбце в таблице.\n",
    "  * Создавать `Series` можно из: списков, словарей, массивов NumPy — всё просто\n",
    "* `DataFrame` — двумерная таблица, каждая колонка — это `Series` с общим индексом.\n",
    "  * Двумерная структура данных (строки и столбцы), каждая колонка — это `Series`. Это основной объект для анализа данных в pandas\n",
    "  * DataFrame можно представить как “словарь” из `Series` с одинаковым индексом, удобно и компактно для таблиц.\n",
    "\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/cdn-uploads/creating_dataframe1.png' width=700>\n",
    "\n",
    "**Полезные ссылки:**  \n",
    "- [Официальная документация pandas](https://pandas.pydata.org/docs/)  \n",
    "- [Быстрое введение: *10 minutes to pandas*](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n",
    "- [Руководство пользователя (*User Guide*)](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html)\n",
    "- [Справочник API (*API Reference*)](https://pandas.pydata.org/docs/reference/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f3bb7",
   "metadata": {},
   "source": [
    "### Задание 1  \n",
    "\n",
    "Сгенерируйте выборку из дискретного равномерного распределения от 0 до 100, размером 2000 и создайте по выборке объект `pd.Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77052211",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc87fa3",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "Сгенерируйте выборку из стандартного нормального распределения, размером 1300 и создайте по выборке объект `pd.Series`. При помощи пропусков, заполните размер до 2000 используя пропуски pd.Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0713d00",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9feb6c",
   "metadata": {},
   "source": [
    "Мы с вами создали объекты `pd.Series` по числовым значениям, но ведь есть еще и другие типы данных. А какие?\n",
    "\n",
    "Удобный список всех типов перечислен [здесь](https://pandas.pydata.org/docs/reference/arrays.html#). \n",
    "\n",
    "Если же перечислять самые основные для начала работы, то это:\n",
    "1. Числовые\n",
    "2. object\n",
    "3. строки + [.str](https://pandas.pydata.org/docs/reference/series.html#api-series-str)\n",
    "4. категории + [.cat](https://pandas.pydata.org/docs/reference/series.html#categorical-accessor)\n",
    "5. даты + [.dt](https://pandas.pydata.org/docs/reference/series.html#datetimelike-properties)\n",
    "6. Nan\n",
    "7. pd Nullable(boolean, integer, float)\n",
    "\n",
    "Запомнить разницу очень легко, если это не числовой тип, то по умолчанию - это object. \n",
    "\n",
    "Кастить типы можно при помощи pd.Series.astype('string') или любой другой тип\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a95c0",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "Сгенерируйте выборку размером 2000 из следующих имен: \"Олег\", \"Андрей\", \"Евгений\", \"Алиса\", \"Мурат\", \"Наталья\", \"Ангелина\", \"Асия\", \"Геогрий\", \"Татьяна\". Оберните полученный массив в объект `pd.Series` и проверьте какой тип получился. Если тип отличен от string, сконвертируйте его к этому типу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe3a7e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e42a86",
   "metadata": {},
   "source": [
    "Существует путаница между `pd.string` и питоновским классом `str`. Главная разница тут в том, что пандас лучше работает со своими типами, и при попытке сконвертировать `pd.Series.astype('string')` тип series будет снова object, не `string`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c16bd",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Сгенерируйте выборку размером 2000 из распределения Бернулли, где с вероятностью 0.01 выдается Nan, и с вероятностью 0.99 с равномерной вероятностью одну из следующих строк: \"Женат/Замужем\", \"Разведен/Разведена\", \"Не женат/Не замужем\". Оберните полученный массив в объект `pd.Series`. Замерьте какое количество памяти занимает текущая серия([подсказка](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.memory_usage.html)). \n",
    "\n",
    "Далее, сконвертируйте в тип `category` и повторите замеры новой серии. Изменилось ли занимаемое место на оперативной памяти?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e42083",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae46859",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "Сгенерируйте выборку размером 2000 из распределения Бернулли, где с вероятностью 0.1 выдается Nan и с вероятностью 0.9 равномерно выдается любая дата из промежутка \"2020-01-01\",\"2021-12-31\". Оберните выборку в `pd.Series` и сконвертируйте тип в `pd.datetime64`\n",
    "\n",
    "Подсказка как можно решить задачу лежит [здесь](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96db77",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df782671",
   "metadata": {},
   "source": [
    "## [Операции с pd.Series](https://pandas.pydata.org/docs/reference/series.html)\n",
    "\n",
    "Для того чтобы использовать какие-то опреации, используется следующая нотация: `pd.Series.func`\n",
    "\n",
    "Возможные опреации:\n",
    "- +-*/&| vs .add, ...\n",
    "- сравнения\n",
    "- `apply, agg, map, transform`\n",
    "- [stats](https://pandas.pydata.org/docs/reference/series.html#computations-descriptive-stats)\n",
    "- `value_counts`\n",
    "- round - округлить\n",
    "- nunique, unique - уникальные элементы\n",
    "- replace - заменить одни значения на другие\n",
    "- dropna - выкинуть пропуски\n",
    "- drop_duplicates - выкинуть дубликаты\n",
    "- sample - посэмлить случайные объекты. учтите, что индекс перемешивается\n",
    "- sort_values, sort_index\n",
    "- `plot`\n",
    "- to_dict - вернуть в качестве словарика {index: value, ...}\n",
    "\n",
    "\n",
    "Отдельно про `apply` - применяет функцию к серии. *Довольно часто* задачу можно решить __без нее__, старайтесь использовать ее только в крайнем случае. Поскольку pandas в основном написан на Cython(в отличае от polars, который на Rust'е), скорость его работы оставляет желать лучшего. Имеющиеся реализованные операции, очень хорошо оптимизированы и позволяют относительно быстро выполнять операции. Чего нельзя будет сказать о рукописной функции, которую вы будете применять\n",
    "\n",
    "Также отделть про `value_counts` - полезно чтобы посмотреть на примерное распределение категориальных(не обязательно) колонок. Часто помогает быстро найти какие-то инсайти, неточности или даже ошибки в данных\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc33bd",
   "metadata": {},
   "source": [
    "### Задание 6\n",
    "Объедините два `pd.Series` полученные на заданиях 1 и 2 в один `pd.DataFrame` объект. Колонку с дискретными значениями из равномерного распределения назвать \"uni_cat_val\", а колонку со значениями из стандартного нормального назвать \"norm_vals_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ddec35",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeea597",
   "metadata": {},
   "source": [
    "## - [операции с pd.DataFrame](https://pandas.pydata.org/docs/reference/frame.html)\n",
    "\n",
    "Для того чтобы использовать какие-то опреации, используется следующая нотация: `pd.DataFrame.func`\n",
    "\n",
    "- Группировать данные(`groupby`)\n",
    "- iterrows - построчный проход по датафрейму(ИСПОЛЬЗУЙТЕ ТОЛЬКО В КРАЙНЕЙ НЕОБХОДИМОСТИ; **_ЛУЧШЕ ИЗБЕГАТЬ_**)\n",
    "- rolling - скользящие окна\n",
    "- query - симпатичный способ общаться с датафреймом, ближе к нотации sql\n",
    "- corr - построить матрицу корреляций признаков\n",
    "- drop - выкинуть какие-то столбцы / строки\n",
    "- drop_duplicates - убрать дубликаты\n",
    "- reset_index - сбросить индекс к RangeIndex(0, N)\n",
    "- sort_values - отсортировать по столбцу\n",
    "- merge - `Merge with a database-style join`. Не используйте `pd.DataFrame.join`!\n",
    "- sample - взять слчайное подмножество указанного размера из датафрейма\n",
    "- assign - присвоить новые столбцы и вернуть результат как новый датафрейм\n",
    "- to_csv, to_excel, to_parquet - сохранить датафрейм в указанный формат\n",
    "\n",
    "Попробуем данные возможности на практикте!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db1993",
   "metadata": {},
   "source": [
    "### Задание 7\n",
    "Сделайте новую колонку, которая будет называться \"is_score_na\", и она будет содержать в себе либо True, либо False. True - если в колонке \"norm_vals_score\" будет None, иначе False. Отфильтруйте только те объекты, у котрых \"is_score_na\" будет False. По ним нужно будет:\n",
    "* посчитать среднее/медиану по \"norm_vals_score\"\n",
    "* сгруппировать данные по колонке \"uni_cat_val\", и посчитать по каждой категории среднее/медиану \"norm_vals_score\"\n",
    "* (*) сделать партицию по колонке \"uni_cat_val\", и посчитать разницу между прошлым и текущим значением колонки \"norm_vals_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7751932",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d3902",
   "metadata": {},
   "source": [
    "Нередко случается такое, что поступили новые данные и их хочется присоединить к текущим, но при этом сделать явную пометку что данные новые"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a4e8a",
   "metadata": {},
   "source": [
    "### Задание 8\n",
    "1. Сгенерируйте выброку из распределения Бернулли, где с вероятностью 0.1 генерируется пропуск, а с вероятностью 0.9 генерируется наблюдение из дискретного равномерного распределения от 0 до 10. Размер выборки должен быь равен 1000 \n",
    "2. Сгенерируйте выброку из распределения Бернулли, где с вероятностью 0.3 генерируется пропуск, а с вероятностью 0.7 генерируется наблюдение из стандартного нормального распределения. Размер выборки должен быь равен 1000 \n",
    "3. Полученные выборки \"оберните\" в объект pd.DataFrame с теми же названиями для колонок и новой колонкой \"is_new_observation\". Для новой колонки проставте константой True\n",
    "4. Теперь, в прошлый датасет, добавьте ту же колонку \"is_new_observation\" и задайте ей значение константно False\n",
    "5. Объедините прошлый датасет, предварительно удалив колонку \"is_score_na\" с новым датасетом. Объединение происходит \"вертикально\", т.е. путем продолжения имеющихся наблюдений\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08c876",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a2f3e",
   "metadata": {},
   "source": [
    "Теперь можно сравнить новые и старые наблюдения. На практите, обычно интересует насколько данные остались однородными. Это довольно глубокая тема, в рамки курса она не попадает, нас же интересует, совпадают ли эмпирические распределения новых и старых данных. Давайте проверим!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd950701",
   "metadata": {},
   "source": [
    "### Задание 9\n",
    "Посчитайте те же статистики, что и в задании 4, но только в разрезе на колонку \"is_new_observation\". Для новых наблюдений, выберите только те объекты у которых ОБЕ колонки не содержат пропусков, далее по этим объектам уже посчитайте среднее/медиану по \"norm_vals_score\", и также по каждой группе \"uni_cat_val\" посчитать/ среднее/медиану \"norm_vals_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646cef3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad0ba4",
   "metadata": {},
   "source": [
    "Как вы наверное могли заметить, с проусками мы пока никак не работали. Исправляем ситуацию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ff6ad",
   "metadata": {},
   "source": [
    "### Задание 10\n",
    "1. Посчитайте абсолютное и относительное количество пропусков в колонках \"norm_vals_score\", \"uni_cat_val\"\n",
    "2. Посчитайте, сколько наблюдений в абсолютных и относительных(от общего размера датасета) числах имеют значение по одной колонке, но пропуск по другой. Т.е. интересует количество ситуаций, когда происходит следующее: uni_cat_val - None, но \"norm_vals_score\" - 0.002 и наоборот\n",
    "3. Заполните пропуски в колонке \"uni_cat_val\" значением \"-1\", а в \"norm_vals_score\" значением -100\n",
    "4. После заполнения пропусков, снова посчитайте среднее и медианное по колонке \"norm_vals_score\" и среднее/медианное по колонке \"norm_vals_score\" аггрегированное до категорий \"uni_cat_val\". Насколько сильно поменялись значения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb8dc3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136acfb6",
   "metadata": {},
   "source": [
    "Дубликаты в данных, это также проблема, которую нужно как-то решать. Ответ на вопрос что делать с дубликатами может звучать по разному, в зависимости от задачи и специфики данных. Если будете углубляться в работу с данными, обязательно держите это в голове. В большинстве случаев просто убирать повторные объекты, это неправильно решение и датасет может потерять какую-то важную информацию.\n",
    "\n",
    "В рамках урока мы об этом не будем задумоваться"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58d0cb",
   "metadata": {},
   "source": [
    "### Задание 11\n",
    "На прошлом датасете, посчитайте абсолютное и относительное количество дубликатов. Дубликаты, это те объекты, у которых полностью совпали значения во всех колонках датасета\n",
    "\n",
    "Уберите дубликаты(объекты, которые встретились в датасете второй раз) из датасета, и посчитайте, наскольк в относительных числах уменшился итоговый датасет. Например, на 7% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5e5ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa993a4",
   "metadata": {},
   "source": [
    "Перейдем к более реальным данным. Скачайте библиотеку seaborn, через pip install и выполните код ячейкой ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa63bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab8199",
   "metadata": {},
   "source": [
    "Это наверное самый популярный датасет из мира машинного обучения и дата аналитики. Он содержит в себе данные всех людей на борту судна титаник, затонувшего в 1912 году. Датасет содержит много признаков, в интернете можно найти описание всех признаков, если интересно. Для нас же, это пока не очень интересно\n",
    "\n",
    "Попытаемся построить интересные подтаблицы и срезы по ней"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58063837",
   "metadata": {},
   "source": [
    "### Задание 12\n",
    "Общий анализ неизвестной таблицы\n",
    "\n",
    "1. Посчитайте количество пропусков в процетном соотношении по каждому столбцу.\n",
    "2. Посчитайте количество дубликатов\n",
    "3. По категориальным колокам, посчитайте моду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7fff1b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077ca36",
   "metadata": {},
   "source": [
    "[Сводные таблицы](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html) - это таблицы, в которых можно аггрегировать данные внутри датафрейма по каким-то другим столбцам(часто категориальным). Очень удобно, когда категорий много и хочется посчитать сразу много статистик по определенным столбцам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e78dc",
   "metadata": {},
   "source": [
    "### Задание 13\n",
    "Постройте сводную таблицу с индексами: class, sex, age. Аггрегация по 'survived'(доле выживших). отсортируйте ее по среднему возрасту мужчин в 1-м классе\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1b77d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ef003",
   "metadata": {},
   "source": [
    "### Задание 14\n",
    "\n",
    "1. Для каждого пассажира вычислите:\n",
    "\n",
    "  * Среднюю стоимость билета(`fare`) по его классу(`class`)\n",
    "  * ранг(оконная функция rank) его билета внутри класса\n",
    "\n",
    "2. Добавьте два новых столбца `fare_mean_class` и `fare_rank_class`\n",
    "3. По столбцам созданными выше, найдите пассажиров с максимальным и минимальным рангом в каждом классе\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8cceb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21080b80",
   "metadata": {},
   "source": [
    "### Задание 15\n",
    "Сгруппируйте данные по (`sex`, `class`) и посчитайте:\n",
    "\n",
    "1. количество пассажиров\n",
    "\n",
    "2. средний возраст\n",
    "\n",
    "3. среднюю стоимость билета\n",
    "\n",
    "\n",
    "После группировки, у полученной таблицы - сбросьте многомерный индекс в обычные колонки и преобразуйте результат так, чтобы строки были по `sex`, а `class` шли в столбцах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c22e13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PASTE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584f6c68",
   "metadata": {},
   "source": [
    "## Введение в polars\n",
    "\n",
    "`polars` - еще один фреймворк для работы с табличными данными, но в отличае от `pandas` обладает рядом преимуществ:\n",
    "1. Быстрая скорость работы. Особенно заметно на больших датасетах(больше 100 млн)\n",
    "2. Более стабильное решение по памяти(иногда чтобы очистить занимаюмаю память pandasом, нативные функции по очистке не помогают и приходится перезагружать ядро)\n",
    "3. На самом деле, два предыдущих пункта это уже просто огромный прорыв, но зачастую когда вы работаете с большими данными, вы работаете со спарком из-за чего привыкаете к его нотации, поларс ее дружелюбно наследует\n",
    "\n",
    "Если он такой крутой, зачем мы проходили pandas?\n",
    "\n",
    "У поларса есть ряд своих недостатков, самые главные:\n",
    "1. Меньше документации и примеров\n",
    "2. Намного меньше интеграций (sklearn, seaborn...) - но всегда можно сделать to_pandas()\n",
    "3. На малых данных не почувствуете разницу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363cde88",
   "metadata": {},
   "source": [
    "После такого небольшого ликбеза, можно переходить к сути\n",
    "\n",
    "* [Официальный сайт](https://pola.rs/)\n",
    "* [Гитхаб с примерами](https://github.com/pola-rs/polars)\n",
    "* [Про типы данных](https://docs.pola.rs/py-polars/html/reference/datatypes.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
